{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Micro (243 LoC) but Complete Generative Pretrained Transformer Model (GPT) for Generating First Names - Data Handling, Training, and Inference\n",
        "\n",
        "\n",
        "## Copyright @Andrej Karpathy\n",
        "\n",
        "This is the code from Andrej Karpathy's [GitHub repository](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95) implementing Micro GPT from scratch in pure Python.\n",
        "\n",
        "## References\n",
        "- https://karpathy.github.io/2026/02/12/microgpt/\n",
        "- https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95\n",
        "- https://www.youtube.com/watch?v=VMj-3S1tku0\n",
        "- https://htmlpreview.github.io/?https://github.com/tanpuekai/microGPT_webEdu/blob/main/index.html\n",
        "- https://genmind.ch/posts/N-gram-Language-Models-The-Classic-Building-Blocks-of-Predictive-Text/\n",
        "- https://lena-voita.github.io/nlp_course/language_modeling.html\n",
        "- Attention is all you Need. Paper - https://arxiv.org/pdf/1706.03762\n",
        "- Transformer Architecture - https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/\n",
        "- Attention is all you Need. Youtube https://www.youtube.com/watch?v=rBCqOTEfxvg\n",
        "- https://jalammar.github.io/illustrated-transformer/\n",
        "- https://jalammar.github.io/illustrated-gpt2/\n",
        "- https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch17/\n",
        "- https://microgpt-academy.vercel.app/\n",
        "\n"
      ],
      "metadata": {
        "id": "iq-MTwimwADn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Internet of Text - Common Crawl, Wikipedia, Facebook, StackExchange, Databases, Documents, Project Gutenberg, E-Books etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "COlmQGKaGYn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Internet of Text](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/InternetofText.jpg?raw=true)\n"
      ],
      "metadata": {
        "id": "jWOWmMfXF6oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text is Sequential. Meaning is determined by position and order of the words in a sentence.\n",
        "\n",
        "\"I went to Delhi.\"   <- Grammatically Correct\n",
        "\n",
        "\"Delhi went to I\"    <- Incorrect\n",
        "\n",
        "## A Para of Text from Wikipedia\n",
        "\n",
        "![A Para of Text From Wikipedia](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/Aparagraphoftext.png?raw=true)"
      ],
      "metadata": {
        "id": "AiNluLMDIuR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Language Model\n",
        "\n",
        "A language model calculates the probability of a sentence or sequence of words  occurring in a natural language, assigning a <b>probability distribution</b> to words.\n",
        "\n",
        "The goal of building langauge models is to determine the most likely next word or sequence of words based on context (Autocomplete).\n",
        "\n",
        "The language models learn from text written by humans (text corpora) by capturing patterns the represent grammatical structure, context and semantic meaning.\n",
        "\n",
        "\n",
        "\n",
        "## Fill in the blanks:\n",
        "\n",
        "Give me a cup of ____?\n",
        "\n",
        "\n",
        "- Coffee 32%\n",
        "- Tea 23%\n",
        "- Water 17%\n",
        "- Milk 10.5%\n",
        "- Rasam 8.2%\n",
        "- Ganji 5.2%\n",
        "- Beer 4%\n",
        "- Earth 0.05%\n",
        "- Soap 0.03%\n",
        "- Keyboard 0.01%\n",
        "- Book 0.01%  \n",
        "\n",
        "# How is the probability distribution computed?\n",
        "\n",
        "\n",
        "![Probabilities](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/probability.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![LanguageModel](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/languagemodel.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![Google Autocomplete](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/GoogleAutocomplete.png?raw=true)\n",
        "\n",
        "## This is a self-supervised learning task.\n",
        "----------------------------------------------------------------\n",
        "\n",
        "# Large Language Models\n",
        "### Large Language Models deep neural networks (Transformer Architecture) trained on enormously Large Corpora of Text (Entire Internet). They learn to predict next work in a sequence of words in natural language really well. They can be fine tuned for multiple downstream tasks.\n",
        "\n",
        "\n",
        "- ChatGPT (version 3.5) was trained on 45 TB of text data. This massive dataset, sourced from web crawls (like Common Crawl), books, Wikipedia, and other internet articles, consisted of over 300 billion words (approx. 500 billion tokens). GPT-4 on over 1 petabyte of data.\n",
        "\n",
        "\n",
        "\n",
        "# Transformer - The original Encoder-Decoder Architecture\n",
        "\n",
        "![Transformer_Original](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/Transformer_attentionisallyouneed.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![TransformerOriginal_Sebastian](https://sebastianraschka.com/images/books/ml-q-and-ai/ch17-fig01.png)\n",
        "\n",
        "The original Transformer Architecture had Encoder blocks as well as Decoder blocks.\n",
        "\n",
        "\n",
        "We can have Encoder only Transformers or Decoder only Transformers.\n",
        "\n",
        "\n",
        "![DecoderOnly](https://towardsdatascience.com/wp-content/uploads/2024/05/1Qww2aaIdqrWVeNmo3AS0ZQ-2048x1314.png)\n",
        "\n",
        "\n",
        "\n",
        "# Generative Pretrained Transformer - Decoder Only Architecture\n",
        "\n",
        "![DecoderOnly_GPT2](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/decoderonly_gpt2.png?raw=true)\n"
      ],
      "metadata": {
        "id": "fP6cg7hhCUr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps in building a Large Language Model using Transformer Deep Neural Network\n",
        "\n",
        "## 1.Data Preparation & Tokenization\n",
        "\n",
        "- Corpus Collection: Gathering trillions of tokens from the web (Common Crawl), books, code (GitHub), and academic papers.\n",
        "\n",
        "- Data Cleaning: Massive filtering to remove \"noise\" (HTML tags, spam), toxic content, and duplicate text.\n",
        "\n",
        "- Tokenization: Breaking text into \"tokens\" (sub-word units). For example, the word \"friendship\" might be split into friend and ship.\n",
        "\n",
        "- Embeddings & Positional Encoding: Each token is converted into a high-dimensional vector. Since Transformers process all tokens in a sentence simultaneously (parallelism), Positional Encodings are added to these vectors so the model knows the order of the words.\n",
        "\n",
        "<b>Simple One Hot Represention of Words into Vectors</b>\n",
        "![WordstoVectors](https://c8j9w8r3.rocketcdn.me/wp-content/uploads/2018/01/one-hot-word-embedding-vectors-768x276.png)\n",
        "\n",
        "\n",
        "<b>Positional Embeddings</b>\n",
        "\n",
        "![PositionalEmbeddings](https://aiml.com/wp-content/uploads/2023/09/example_of_positional_encoding_in_transformers.png)\n",
        "\n",
        "\n",
        "\n",
        "## 2. Self-Supervised Pre-training : Learning to Predict the Next Token\n",
        "\n",
        "- This is the most expensive and time-consuming stage of training language models. This uses Self-Supervised Learning, no human labels are needed; the text itself provides the \"answers.\"\n",
        "\n",
        "- Objective of this training is Next-Token Prediction. The model is given a sequence and must guess the next word.\n",
        "\n",
        "- The model uses Self-Attention to determine which words in a sentence are relevant to others (e.g., in \"The cat sat on the mat because it was tired,\" attention helps the model link \"it\" to \"cat\").\n",
        "\n",
        "- This produces a Base Model (like Llama 3) that knows facts and grammar but doesn't yet know how to follow instructions.\n",
        "\n",
        "## 3. Supervised Fine-Tuning (SFT)\n",
        "- SFT teaches the base model how to respond to prompts.\n",
        "\n",
        "\n",
        "## 4. Alignment & RLHF (The \"Safety\" Phase)\n",
        "- Even after SFT, a model might be rude, biased, or confidently wrong. Alignment ensures the model's values match human expectations. This is done using preference labeling, reward model, reinforcement learning (PPO, DPO etc.).\n"
      ],
      "metadata": {
        "id": "WFW_rUWSV0VB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Walkthrough of Andrej Karpathy's Code for microGPT\n",
        "\n",
        "https://karpathy.github.io/2026/02/12/microgpt/\n",
        "\n",
        "\n",
        "In this code, we are implementing the self-supervised pretraining of a very small GPT-2 architecture. The steps we follow include:\n",
        "\n",
        "1. Data Ingestion, the dataset we use is already clean.\n",
        "2. Write a tokenizer for our data and tokenize our data\n",
        "3. Write our automatic differentation class (AutoGrad) that computes local gradient at each node in the neural network, it has backward() to propagate the gradients backwards in the NN.\n",
        "4. Initialize the hyper-parameters for our neural network\n",
        "5. Define the layers and their placement in our neural network (architecture design)\n",
        "6. Write our Adam optimizer for weight update of our NN at each step of gradient calculation\n",
        "7. Train our model on our dataset\n",
        "8. Use the model for inference - Generate new first names for us.\n"
      ],
      "metadata": {
        "id": "eTJln9WWS6N7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import required Python libraries"
      ],
      "metadata": {
        "id": "hpxOlU5QwfK_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XKbO4D0Mvc0v"
      },
      "outputs": [],
      "source": [
        "import os       # os.path.exists\n",
        "import math     # math.log, math.exp\n",
        "import random   # random.seed, random.choices, random.gauss, random.shuffle\n",
        "random.seed(42) # Let there be order among chaos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create a text dataset to train out language model on\n",
        "\n",
        "The data is coming from names.txt file shared by Andrej Karpathy:\n",
        "\n",
        "https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt\n",
        "\n",
        "- The file contains 32033 names (first names) of human beings.\n",
        "\n",
        "- Code below, reads the file names.txt at above URL, stores its content in a file called input.txt and then strips the spaces around each line of text.\n",
        "\n",
        "- Then it randomly shuffles the contents.\n",
        "\n",
        "Here is what the dataset looks like:\n",
        "\n",
        "![DatasetContents](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/dataset.png?raw=true)\n"
      ],
      "metadata": {
        "id": "SiFohCqVwiN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let there be a Dataset `docs`: list[str] of documents (e.g. a list of names)\n",
        "if not os.path.exists('input.txt'):\n",
        "    import urllib.request\n",
        "    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt'\n",
        "    urllib.request.urlretrieve(names_url, 'input.txt')\n",
        "docs = [line.strip() for line in open('input.txt') if line.strip()]\n",
        "random.shuffle(docs)\n",
        "print(f\"num docs: {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpzxiyEQwlZV",
        "outputId": "aa0dc19d-13da-4190-87f9-7feb08790059"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num docs: 32033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HS2LQTG-XL8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create a tokenizer to convert strings (representing names) to sequences of integers(tokens) and back.\n",
        "\n",
        "- Unique characters in the dataset become token ids 0 to n-1 (There are 27 unique characters in English language)\n",
        "- BOS -  is a special token representing Beginning of Sequence\n",
        "- Vocabulary consists of unique tokens in the dataset\n",
        "- Vocabulary size is the number of unique tokens + 1 (BOS)"
      ],
      "metadata": {
        "id": "PxcmuhvnxSsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let there be a Tokenizer to translate strings to sequences of integers (\"tokens\") and back\n",
        "uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1\n",
        "BOS = len(uchars) # token id for a special Beginning of Sequence (BOS) token\n",
        "vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS\n",
        "print(f\"vocab size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WRKxbwmxVqd",
        "outputId": "69d1e951-c600-4efb-887f-0ba3ff83e2e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Build a class for calculating gradient (automatic differentiation) of the loss function and pass the gradient backwards in the neural network (computational graph) for weight updation\n",
        "\n",
        "# For a function with one output and many inputs, we calculate the Gradient:\n",
        "\n",
        "![Gradient](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/Gradient.png?raw=true)\n",
        "\n",
        "# For a function with multiple outputs and multiple inputs, we calculate the Jacobian:\n",
        "\n",
        "\n",
        "![Jacobian](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/Jacobian.png?raw=true)\n",
        "\n",
        "\n",
        "# Chain Rule of Calculus helps us compute Gradients at each Node in the NN:\n",
        "\n",
        "![ChainRule](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/ChainRule.png?raw=true)\n",
        "\n",
        "The gradient of loss function is propagated backwards from output node of the neural network towards first hidden layer, and the weights of all nodes are updated using an Optimizer such as Adam, RMSProp, SGD etc.\n",
        "\n",
        "![BackPropagation](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/Backpropagation.png?raw=true)"
      ],
      "metadata": {
        "id": "sWKal_ttyKaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MicroGPT Architecture\n",
        "\n",
        "![MicroGPT Architecture](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/ModelArchitecture_microGPT.png?raw=true)\n",
        "\n"
      ],
      "metadata": {
        "id": "Sju3PQLOkIAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value Class\n",
        "![ValueClass](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/ValueClass.png?raw=true)"
      ],
      "metadata": {
        "id": "K_wBbYb-kg8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let there be Autograd to recursively apply the chain rule through a computation graph\n",
        "class Value:\n",
        "    __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage\n",
        "\n",
        "    def __init__(self, data, children=(), local_grads=()):\n",
        "        self.data = data                # scalar value of this node calculated during forward pass\n",
        "        self.grad = 0                   # derivative of the loss w.r.t. this node, calculated in backward pass\n",
        "        self._children = children       # children of this node in the computation graph\n",
        "        self._local_grads = local_grads # local derivative of this node w.r.t. its children\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data + other.data, (self, other), (1, 1))\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data * other.data, (self, other), (other.data, self.data))\n",
        "\n",
        "    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))\n",
        "    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))\n",
        "    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
        "    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))\n",
        "    def __neg__(self): return self * -1\n",
        "    def __radd__(self, other): return self + other\n",
        "    def __sub__(self, other): return self + (-other)\n",
        "    def __rsub__(self, other): return other + (-self)\n",
        "    def __rmul__(self, other): return self * other\n",
        "    def __truediv__(self, other): return self * other**-1\n",
        "    def __rtruediv__(self, other): return other * self**-1\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._children:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            for child, local_grad in zip(v._children, v._local_grads):\n",
        "                child.grad += local_grad * v.grad"
      ],
      "metadata": {
        "id": "v7vfz24LyI_t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Initialize the parameters of microGPT"
      ],
      "metadata": {
        "id": "r2zT-SMQyoW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the parameters, to store the knowledge of the model\n",
        "n_layer = 1     # depth of the transformer neural network (number of layers)\n",
        "n_embd = 16     # width of the network (embedding dimension)\n",
        "block_size = 16 # maximum context length of the attention window (note: the longest name is 15 characters)\n",
        "n_head = 4      # number of attention heads\n",
        "head_dim = n_embd // n_head # derived dimension of each head\n",
        "matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n",
        "state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}\n",
        "for i in range(n_layer):\n",
        "    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)\n",
        "params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]\n",
        "print(f\"num params: {len(params)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7RsN23Gynf0",
        "outputId": "e4033ebb-cd3d-4fe7-a857-b763d4bf42b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num params: 4192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Define the model architecture (a model is a function mapping tokens and parameters to logits over what comes next in a sequence)\n",
        "\n",
        "This model follows the architecture of Generative Pretrained Transformer -2, with minor differences:\n",
        "- layernorm is replaced by rmsnorm,\n",
        "- there are no biases,\n",
        "- GeLU is replaced by ReLU\n",
        "\n",
        "# Multiheaded Attention Loop in MicroGPT\n",
        "\n",
        "![MultiHeadedAttention](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/MultiHeadedAttentionLoop.png?raw=true)"
      ],
      "metadata": {
        "id": "CDfUkHrnyIqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear Layer\n",
        "def linear(x, w):\n",
        "    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n",
        "\n",
        "\n",
        "#Softmax Layer\n",
        "def softmax(logits):\n",
        "    max_val = max(val.data for val in logits)\n",
        "    exps = [(val - max_val).exp() for val in logits]\n",
        "    total = sum(exps)\n",
        "    return [e / total for e in exps]\n",
        "\n",
        "\n",
        "# RMSNorm Layer\n",
        "def rmsnorm(x):\n",
        "    ms = sum(xi * xi for xi in x) / len(x)\n",
        "    scale = (ms + 1e-5) ** -0.5\n",
        "    return [xi * scale for xi in x]\n",
        "\n",
        "\n",
        "# Complete Transformer Architecture\n",
        "def gpt(token_id, pos_id, keys, values):\n",
        "    tok_emb = state_dict['wte'][token_id] # token embedding\n",
        "    pos_emb = state_dict['wpe'][pos_id] # position embedding\n",
        "    x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding\n",
        "    x = rmsnorm(x) # note: not redundant due to backward pass via the residual connection\n",
        "\n",
        "    for li in range(n_layer):\n",
        "        # 1) Multi-head Attention block\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n",
        "        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n",
        "        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n",
        "        #Keys and values\n",
        "        keys[li].append(k)\n",
        "        values[li].append(v)\n",
        "        x_attn = []\n",
        "        for h in range(n_head):\n",
        "            hs = h * head_dim\n",
        "            q_h = q[hs:hs+head_dim]\n",
        "            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\n",
        "            v_h = [vi[hs:hs+head_dim] for vi in values[li]]\n",
        "            #Calculating attention scores\n",
        "            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]\n",
        "            attn_weights = softmax(attn_logits)\n",
        "            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]\n",
        "            x_attn.extend(head_out)\n",
        "        #Apply linear layer to attention scores\n",
        "        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "\n",
        "\n",
        "        # 2) MLP block\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n",
        "        x = [xi.relu() for xi in x]\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "\n",
        "    logits = linear(x, state_dict['lm_head'])\n",
        "    return logits"
      ],
      "metadata": {
        "id": "xyIdkV3ry_qM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training Loop\n",
        "\n",
        "- Define the Adam optimizer for updating the model parameters at each step of gradient calculation\n",
        "\n",
        "\n",
        "![TrainingLoop](https://github.com/NimritaKoul/microGPTStudyGroup/blob/main/TrainingLoop.png?raw=true)"
      ],
      "metadata": {
        "id": "uzxK0zn-zbLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8\n",
        "m = [0.0] * len(params) # first moment buffer\n",
        "v = [0.0] * len(params) # second moment buffer\n",
        "\n",
        "# Repeat in sequence\n",
        "num_steps = 1000 # number of training steps\n",
        "for step in range(num_steps):\n",
        "\n",
        "    # Take single document, tokenize it, surround it with BOS special token on both sides\n",
        "    doc = docs[step % len(docs)]\n",
        "    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n",
        "    n = min(block_size, len(tokens) - 1)\n",
        "\n",
        "    # Forward the token sequence through the model, building up the computation graph all the way to the loss\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    losses = []\n",
        "    for pos_id in range(n):\n",
        "        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax(logits)\n",
        "        loss_t = -probs[target_id].log()\n",
        "        losses.append(loss_t)\n",
        "    loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.\n",
        "\n",
        "    # Backward the loss, calculating the gradients with respect to all model parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Adam optimizer update: update the model parameters based on the corresponding gradients\n",
        "    lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay\n",
        "    for i, p in enumerate(params):\n",
        "        m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n",
        "        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\n",
        "        m_hat = m[i] / (1 - beta1 ** (step + 1))\n",
        "        v_hat = v[i] / (1 - beta2 ** (step + 1))\n",
        "        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\n",
        "        p.grad = 0\n",
        "\n",
        "    print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\", end='\\r')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ScHJtZ-zam8",
        "outputId": "edda612a-bd67-48f9-bb0b-752f0113379b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Use the model for inference - it will generate new names similar to the names in training data."
      ],
      "metadata": {
        "id": "Jc2UERpazaZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference: may the model babble back to us\n",
        "temperature = 0.5 # in (0, 1], control the \"creativity\" of generated text, low to high\n",
        "print(\"\\n--- inference (new, hallucinated names) ---\")\n",
        "for sample_idx in range(20):\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    token_id = BOS\n",
        "    sample = []\n",
        "    for pos_id in range(block_size):\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax([l / temperature for l in logits])\n",
        "        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n",
        "        if token_id == BOS:\n",
        "            break\n",
        "        sample.append(uchars[token_id])\n",
        "    print(f\"sample {sample_idx+1:2d}: {''.join(sample)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDcSUsvTzw4E",
        "outputId": "10d62b70-e5a3-4596-afce-fcd32c8f8d6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- inference (new, hallucinated names) ---\n",
            "sample  1: kamon\n",
            "sample  2: ann\n",
            "sample  3: karai\n",
            "sample  4: jaire\n",
            "sample  5: vialan\n",
            "sample  6: karia\n",
            "sample  7: yeran\n",
            "sample  8: anna\n",
            "sample  9: areli\n",
            "sample 10: kaina\n",
            "sample 11: konna\n",
            "sample 12: keylen\n",
            "sample 13: liole\n",
            "sample 14: alerin\n",
            "sample 15: earan\n",
            "sample 16: lenne\n",
            "sample 17: kana\n",
            "sample 18: lara\n",
            "sample 19: alela\n",
            "sample 20: anton\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank You\n",
        "# In further sessions, we will dive deep into each of the above code blocks of microGPT architecture.\n"
      ],
      "metadata": {
        "id": "EoVEEFP9l2y8"
      }
    }
  ]
}